{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kibbl\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pypdf                # PDF reader\n",
    "from tqdm import tqdm       # Progress bar bc I'm impatient\n",
    "import os                   # Navigate folders\n",
    "import time                 # Timing\n",
    "import tracemalloc          # Memory Usage\n",
    "\n",
    "import re                   # Text preprocessing stuff\n",
    "import string               # More text preprocessing\n",
    "import nltk                 # Tokenization\n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer       # Embedding Model\n",
    "\n",
    "from collections import Counter                             # Simple counting dictionary\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Roland Notes\n",
    "# reader = pypdf.PdfReader('Roland_Notes.pdf')\n",
    "\n",
    "# notes = ''\n",
    "\n",
    "# for page in tqdm(reader.pages):\n",
    "#     notes += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read in slides\n",
    "texts = []\n",
    "\n",
    "for doc in tqdm(os.listdir('Slides')):\n",
    "    reader = pypdf.PdfReader(f'Slides/{doc}')\n",
    "\n",
    "    text = ''\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    texts.append(text)\n",
    "\n",
    "text = ' '.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper preprocessing functions\n",
    "\n",
    "def normalize_text(text, case_senstive=False):\n",
    "\n",
    "    # Normalizes case if need be\n",
    "    if case_senstive:\n",
    "        text = text.lower()\n",
    "\n",
    "    # Removes whitespace\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, method='word'):\n",
    "\n",
    "    # Very basic text normalization\n",
    "    text = normalize_text(text)\n",
    "\n",
    "    if method == 'word':\n",
    "    \n",
    "        # Tokenization\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords if need be\n",
    "        tokens = remove_stopwords(tokens)\n",
    "\n",
    "        # Replaces wacky symbols (like stylized bullets) with <SYM> token if need be\n",
    "        tokens = [\"<SYM>\" if re.fullmatch(r\"[^\\w\\d\" + re.escape(string.punctuation) + \"]\", token) else token for token in tokens]\n",
    "\n",
    "        # Replaces words that show up only once with <UNK> token if need be\n",
    "        # rare = [item[0] for item in Counter(tokens).items() if item[1] == 1]\n",
    "        # tokens = ['<UNK>' if token in rare else token for token in tokens]\n",
    "\n",
    "        # Replaces pure numbers with <NUM> token if need be\n",
    "        tokens = ['<NUM>' if token.isdigit() else token for token in tokens]\n",
    "\n",
    "        # Removes punctuation marks\n",
    "        # tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    elif method == 'sent':\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "        # Preprocessing similar to regular word preprocessing if need be\n",
    "        for i in range(len(tokens)):\n",
    "            sent = tokens[i]\n",
    "            sent = ' '.join(preprocess_text(sent))\n",
    "            tokens[i] = sent\n",
    "            \n",
    "\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Function for chunking text\n",
    "def chunk_text(text, chunk_size, overlap=0):\n",
    "    chunks = []\n",
    "    for start in range(len(text) // (chunk_size-overlap) + 1):\n",
    "        chunks.append(text[start * (chunk_size-overlap) : (start+1) * (chunk_size-overlap)])\n",
    "    return chunks\n",
    "\n",
    "tokens = preprocess_text(text, 'sent')\n",
    "chunks = chunk_text(text, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:13<00:00,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 136.1569 seconds\n",
      "Peak memory usage: 12.43 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding function\n",
    "def embed_tokens(chunks, model):    \n",
    "    embeddings = []\n",
    "    for chunk in tqdm(chunks):\n",
    "        slay = model.encode(chunks)\n",
    "        embeddings.append(slay)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Test model 1\n",
    "\n",
    "tracemalloc.start()\n",
    "start_time = time.time()\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = embed_tokens(chunks[:20], model)\n",
    "\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f'Time elapsed: {round(elapsed, 4)} seconds')\n",
    "print(f\"Peak memory usage: {peak / 1024**2:.2f} MiB\")\n",
    "\n",
    "# Test model 2\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# embeddings = embed_tokens(chunks[:20], model)\n",
    "\n",
    "# Test model 3\n",
    "# model = SentenceTransformer(\"hkunlp/instructor-xl\")\n",
    "# embeddings = []\n",
    "# for chunk in tqdm(chunks[:10]):\n",
    "#     instruction = 'Represent the Data Science sentence for retrieval: '\n",
    "#     corpus = [[instruction, sent] for sent in chunk]\n",
    "#     embed = model.encode(corpus)\n",
    "\n",
    "#     embeddings.append(embed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
