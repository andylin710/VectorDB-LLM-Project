{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:42: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:42: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\kibbl\\AppData\\Local\\Temp\\ipykernel_33360\\1372472663.py:42: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\"\"\n",
      "c:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kibbl\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## DS 4300 Example - from docs\n",
    "import re                   # Text preprocessing stuff\n",
    "import string               # More text preprocessing\n",
    "import nltk                 # Tokenization\n",
    "import csv                  # CSV writing\n",
    "\n",
    "import ollama               # Ollama\n",
    "import redis                # Redis\n",
    "import numpy as np          # Numpy\n",
    "import fitz                 # PDF Reader\n",
    "\n",
    "from tqdm import tqdm       # Progress bar bc I'm impatient\n",
    "import os                   # Navigate folders\n",
    "import time                 # Timing\n",
    "import tracemalloc          # Memory Usage\n",
    "import cpuinfo              # CPU Info\n",
    "import psutil               # Memory Info\n",
    "\n",
    "from sentence_transformers import SentenceTransformer       # Embedding Model\n",
    "from collections import Counter                             # Simple counting dictionary\n",
    "from redis.commands.search.query import Query               # Querying \n",
    "from redis.commands.search.field import VectorField, TextField\n",
    "\n",
    "# Libraries for nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Important constants\n",
    "VECTOR_DIM = 768\n",
    "INDEX_NAME = \"embedding_index\"\n",
    "DOC_PREFIX = \"slides\"\n",
    "DISTANCE_METRIC = \"COSINE\"\n",
    "QUERY = 'What is the CAP Theorem?'\n",
    "LLM_MODEL = 'llama3.2:latest'\n",
    "EMBEDDING_MODEL = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "# Initialize Redis connection\n",
    "redis_client = redis.Redis(host=\"localhost\", port=6379, db=0, decode_responses=True)\n",
    "\n",
    "def clear_redis_store():\n",
    "    \"\"\"\n",
    "    Clears redis database store to prevent overlap\n",
    "\n",
    "    Parameters:         C              A\n",
    "        None           / \\            1 C\n",
    "                      A   3            2 3 \n",
    "    Returns:         1 2                LL\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Clearing existing Redis store...\")\n",
    "    redis_client.flushdb()\n",
    "    print(\"Redis store cleared.\")\n",
    "\n",
    "\n",
    "def create_hnsw_index():\n",
    "    \"\"\"\n",
    "    Clears an HNSW index in the Redis database\n",
    "\n",
    "    Parameters:          C                 C               B\n",
    "        None           A   3            B     3         A     C\n",
    "                      1 B             A   R             \n",
    "    Returns:           L R           1 L  \n",
    "        None\n",
    "    \"\"\"\n",
    "    # Removes index if it already exists\n",
    "    try:\n",
    "        redis_client.execute_command(f\"FT.DROPINDEX {INDEX_NAME} DD\")\n",
    "    except redis.exceptions.ResponseError:\n",
    "        pass\n",
    "\n",
    "    # Creates index\n",
    "    redis_client.execute_command(\n",
    "        f\"\"\"\n",
    "        FT.CREATE {INDEX_NAME} ON HASH PREFIX 1 {DOC_PREFIX}\n",
    "        SCHEMA text TEXT\n",
    "        embedding VECTOR HNSW 6 DIM {VECTOR_DIM} TYPE FLOAT32 DISTANCE_METRIC {DISTANCE_METRIC}\n",
    "        \"\"\"\n",
    "    )\n",
    "    print(\"Index created successfully.\")\n",
    "\n",
    "\n",
    "def get_embedding(text: str, embedding_model) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding via a specified model.    B+ tree n-1 keys -> n children, default make nodes half full\n",
    "                                                    Parents are lowest values of right n-1 keys\n",
    "    Parameters:\n",
    "        text (str): Text to embed\n",
    "        embedding_model (str): Name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        response (np.Array): Numerical array representation of the embeddings\n",
    "    \n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(embedding_model, trust_remote_code=True)\n",
    "    response = model.encode(text)\n",
    "    return response\n",
    "\n",
    "\n",
    "def store_embedding(file: str, page: str, chunk: str, embedding: list):\n",
    "    \"\"\"\n",
    "    Stores the embeddings in the Redis index        Hashtable: Apply hashing function to key, insert into hashed\n",
    "                                                    index a tuple of (non-hash key, val), linked list if multiple\n",
    "    Parameters:                                     lambda = num inserts / num hashes, should be < 0.9\n",
    "        file (str): Name of the file for indexing\n",
    "        page (str): Page number of the file\n",
    "        chunk (str): Chunk number of the file\n",
    "        embedding (list): Embedding representation of a single chunk\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    key = f\"{DOC_PREFIX}:{file}_page_{page}_chunk_{chunk}\"\n",
    "    redis_client.hset(\n",
    "        key,\n",
    "        mapping={\n",
    "            \"file\": file,\n",
    "            \"page\": page,\n",
    "            \"chunk\": chunk,\n",
    "            \"embedding\": np.array(\n",
    "                embedding, dtype=np.float32\n",
    "            ).tobytes(),  \n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text by page from the pdf               ACID: Atomicity, Consistency, Isolation, Durability\n",
    "                                                        everything as unit, stable state to stable state, \n",
    "    Parameters:                                         transactions are independent, transactions are permamet\n",
    "        pdf_path (str): Path to the pdf file            even at system failure\n",
    "\n",
    "    Returns:\n",
    "        text_by_page (list): List of the text on each page\n",
    "    \"\"\"\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_by_page = []\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text_by_page.append((page_num, page.get_text()))\n",
    "    return text_by_page\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses and tokenizes text. Steps can be commented out if need be. \n",
    "\n",
    "    Transaction: CRUD operations performed as a single unit of work. All work or all abort\n",
    "    Dirty read, Non-repeatable read, phantom read (reference data not existing)\n",
    "    Scale up, then out by using distributed system that can withstand conflicts\n",
    "\n",
    "    CAP: Systems are either consistent, available, or partition tolerant\n",
    "    Identical views, present during failure, work in different segments\n",
    "\n",
    "    Parameters:                                 \n",
    "        text (str): Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "        tokens (list): List of tokenized words\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace new lines\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "\n",
    "    # Normalize case\n",
    "    # text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords if need be\n",
    "    # tokens = remove_stopwords(tokens)\n",
    "\n",
    "    # Replaces wacky symbols (like stylized bullets) with <SYM> token if need be\n",
    "    tokens = [\"<SYM>\" if re.fullmatch(r\"[^\\w\\d\" + re.escape(string.punctuation) + \"]\", token) else token for token in tokens]\n",
    "\n",
    "    # Replaces words that show up only once with <UNK> token if need be\n",
    "    # rare = [item[0] for item in Counter(tokens).items() if item[1] == 1]\n",
    "    # tokens = ['<UNK>' if token in rare else token for token in tokens]\n",
    "\n",
    "    # Replaces pure numbers with <NUM> token if need be\n",
    "    # tokens = ['<NUM>' if token.isdigit() else token for token in tokens]\n",
    "\n",
    "    # Removes punctuation marks\n",
    "    # tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "\n",
    "    # KV is very simple, quick o(1), horizontally scalable, example being redis\n",
    "    # Easily store model features, intermediate results\n",
    "    return tokens\n",
    "\n",
    "# split the text into chunks with overlap\n",
    "def split_text_into_chunks(text, chunk_size=300, overlap=50):\n",
    "    \"\"\"Split text into chunks of approximately chunk_size words with overlap.\n",
    "    \n",
    "    BASE: Basically available, soft state, eventual consistency\n",
    "    Pessimistic: Something could wil go wrong\n",
    "    Optimistic: If something happens, should be ok\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i : i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Process all PDF files in a given directory, returns elapsed time and peak memory\n",
    "def process_pdfs(data_dir, model, chunk_size=300, overlap=50):\n",
    "    \"\"\"\n",
    "    INCR somevalue\n",
    "    INCRBY somevalue N\n",
    "    SET _ N\n",
    "    GET \n",
    "\n",
    "    Has hash and linked list structures\n",
    "    Supports sets, json\n",
    "    \"\"\"\n",
    "\n",
    "    # Start time / memory check\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for file_name in tqdm(os.listdir(data_dir)):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(data_dir, file_name)\n",
    "            text_by_page = extract_text_from_pdf(pdf_path)\n",
    "            for page_num, text in text_by_page:\n",
    "                chunks = split_text_into_chunks(text, chunk_size, overlap)\n",
    "                for chunk_index, chunk in enumerate(chunks):\n",
    "                    embedding = get_embedding(chunk, model)\n",
    "                    store_embedding(\n",
    "                        file=file_name,\n",
    "                        page=str(page_num),\n",
    "                        chunk=str(chunk),\n",
    "                        embedding=embedding,\n",
    "                    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    print(f'Time elapsed: {round(elapsed, 4)} seconds')\n",
    "    print(f\"Peak memory usage: {peak / 1024**2:.2f} MiB\")\n",
    "\n",
    "    # returns time and peak memory\n",
    "    return round(elapsed, 4), round((peak / 1024**2), 2)\n",
    "\n",
    "\n",
    "def search_embeddings(query, model, top_k=3):\n",
    "    \"\"\"\n",
    "    json lightweight, easy for humans and machines, many uses\n",
    "    bson is binary serialization of json, allows working with more types + minimize space\n",
    "    xml is older version like json, uses tags like html\n",
    "    Doc datab is good for more oo design + complex structuers that are self describing\n",
    "    \"\"\"\n",
    "\n",
    "    query_embedding = get_embedding(query, model)\n",
    "\n",
    "    # Convert embedding to bytes for Redis search\n",
    "    query_vector = np.array(query_embedding, dtype=np.float32).tobytes()\n",
    "\n",
    "    try:\n",
    "        q = (\n",
    "            Query(\"*=>[KNN 5 @embedding $vec AS vector_distance]\")\n",
    "            .sort_by(\"vector_distance\")\n",
    "            .return_fields(\"id\", \"file\", \"page\", \"chunk\", \"vector_distance\")\n",
    "            .dialect(2)\n",
    "        )\n",
    "\n",
    "        # Perform the search\n",
    "        results = redis_client.ft(INDEX_NAME).search(\n",
    "            q, query_params={\"vec\": query_vector}\n",
    "        )\n",
    "\n",
    "        # Transform results into the expected format\n",
    "        top_results = [\n",
    "            {\n",
    "                \"file\": result.file,\n",
    "                \"page\": result.page,\n",
    "                \"chunk\": result.chunk,\n",
    "                \"similarity\": result.vector_distance,\n",
    "            }\n",
    "            for result in results.docs\n",
    "        ][:top_k]\n",
    "\n",
    "        # Print results for debugging\n",
    "        # for result in top_results:\n",
    "        #     print(\n",
    "        #         f\"---> File: {result['file']}, Page: {result['page']}, Chunk: {result['chunk']}\"\n",
    "        #     )\n",
    "\n",
    "        return top_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "        return []\n",
    "    \n",
    "def generate_rag_response(query, context_results, model):\n",
    "    \"\"\"\n",
    "    MongoDB has lots of query support, idnexing, replication\n",
    "    db.users.find({'key': 'value'})\n",
    "    \"imdb.rating\": { $gte: 7 } \n",
    "    $eq, $gt, $ne, $in, $nin\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare context string\n",
    "    context_str = \"\\n\".join(\n",
    "        [\n",
    "            f\"From {result.get('file', 'Unknown file')} (page {result.get('page', 'Unknown page')}, chunk {result.get('chunk', 'Unknown chunk')}) \"\n",
    "            f\"with similarity {float(result.get('similarity', 0)):.2f}\"\n",
    "            for result in context_results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Construct prompt with context\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. \n",
    "    Use the following context to answer the query as accurately as possible. If the context is \n",
    "    not relevant to the query, say 'I don't know'.\n",
    "\n",
    "    Context:\n",
    "    {context_str}\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    # Generate response using Ollama\n",
    "    response = ollama.chat(\n",
    "        model=model, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# Function to detect CPU type\n",
    "def get_cpu_type():\n",
    "    cpu_brand = cpuinfo.get_cpu_info()['brand_raw']\n",
    "    return cpu_brand \n",
    "\n",
    "# Function to detect RAM size\n",
    "def get_ram_size():\n",
    "    return round(psutil.virtual_memory().total / (1024 ** 3))\n",
    "\n",
    "# IMPORT THIS\n",
    "def run_test(queries, embedding_model, llm_model, chunk_size=300, overlap=50):\n",
    "    redis_client = redis.Redis(host=\"localhost\", port=6379, db=0, decode_responses=True)\n",
    "    answers = []\n",
    "\n",
    "    clear_redis_store()\n",
    "    create_hnsw_index()\n",
    "\n",
    "    print('Processing PDFs...')\n",
    "    index_elapsed, index_memory = process_pdfs(\"All_Slides/\", embedding_model, chunk_size, overlap)\n",
    "    print(\"\\n---Done processing PDFs---\\n\")\n",
    "\n",
    "    # define csv file\n",
    "    csv_filename = \"roland_redis_test_results.csv\"\n",
    "\n",
    "    with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header only if the file has no data\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"compute_type\", \"memory_size\", \"embedding_model\", \"llm_model\", \"index_elapsed\", \"index_memory\", \"query\", \"query_time_elapsed\", 'chunk_size', 'overlap'])\n",
    "\n",
    "        for query in queries:\n",
    "            print('Query:', query)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Generate response\n",
    "            response = generate_rag_response(query, search_embeddings(query, embedding_model), llm_model)\n",
    "            print(response)\n",
    "            answers.append(response)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'Time elapsed: {round(elapsed, 4)} seconds')\n",
    "            print('---------------------------')\n",
    "\n",
    "            cpu_type = get_cpu_type()\n",
    "            ram_size = get_ram_size()\n",
    "\n",
    "            # Write data row to CSV\n",
    "            writer.writerow([cpu_type, ram_size, embedding_model, llm_model, index_elapsed, index_memory, query, round(elapsed, 4), chunk_size, overlap])\n",
    "\n",
    "    print(f\"Results saved to {csv_filename}\")\n",
    "\n",
    "    answers = '\\n------------------------\\n'.join(answers)\n",
    "    with open(f\"QUERY RESULTS_Redis_{embedding_model.split('/')[1]}_{llm_model.replace('.', '_').replace(':', '_')}_{chunk_size}_{overlap}.txt\", 'w') as file:\n",
    "        file.write(answers)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing Redis store...\n",
      "Redis store cleared.\n",
      "Index created successfully.\n",
      "Processing PDFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [08:17<00:00, 31.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 497.1413 seconds\n",
      "Peak memory usage: 38.58 MiB\n",
      "\n",
      "---Done processing PDFs---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "redis_client = redis.Redis(host=\"localhost\", port=6379, db=0, decode_responses=True)\n",
    "answers = []\n",
    "\n",
    "clear_redis_store()\n",
    "create_hnsw_index()\n",
    "\n",
    "print('Processing PDFs...')\n",
    "index_elapsed, index_memory = process_pdfs(\"All_Slides/\", EMBEDDING_MODEL)\n",
    "print(\"\\n---Done processing PDFs---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the query you can use to achieve the desired result:\n",
      "\n",
      "```javascript\n",
      "db.university.students.find({\n",
      "  \"courses_completed\": {\n",
      "    \"$elemMatch\": {\n",
      "      \"course_name\": \"Intro to Big Data\",\n",
      "      \"semester\": \"Spring 2023\",\n",
      "      \"grade\": { $gte: \"B+\" }\n",
      "    }\n",
      "  }\n",
      "}, {\n",
      "  \"first_name\": 1,\n",
      "  \"last_name\": 1,\n",
      "  \"email\": 1\n",
      "}).sort({ last_name: 1, first_name: 1 });\n",
      "```\n",
      "\n",
      "This query filters the documents to include only those where a course with name \"Intro to Big Data\", semester \"Spring 2023\", and grade at least \"B+\" exists. It then selects only the `first_name`, `last_name`, and `email` fields for each matching document, and finally sorts the results by last name and then first name in ascending order (1).\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "Assume you are presented with a collection of documents structured like the following example.  Assume the database is named university and the collection is named students.\n",
    "\n",
    "{\n",
    "  \"student_id\": \"S123456789\",\n",
    "  \"first_name\": \"Alex\",\n",
    "  \"last_name\": \"Johnson\",\n",
    "  \"email\": \"alex.johnson@example.edu\",\n",
    "  \"enrollment_year\": 2023,\n",
    "  \"major\": \"Computer Science\",\n",
    "  \"courses_completed\": [\n",
    "    {\n",
    "      \"course_id\": \"CS101\",\n",
    "      \"course_name\": \"Introduction to Programming\",\n",
    "      \"semester\": \"Fall 2023\",\n",
    "      \"credits\": 4,\n",
    "      \"grade\": \"A-\"\n",
    "    },\n",
    "    {\n",
    "      \"course_id\": \"MATH204\",\n",
    "      \"course_name\": \"Calculus II\",\n",
    "      \"semester\": \"Fall 2023\",\n",
    "      \"credits\": 4,\n",
    "      \"grade\": \"B+\"\n",
    "    },\n",
    "    {\n",
    "      \"course_id\": \"ENG150\",\n",
    "      \"course_name\": \"Academic Writing\",\n",
    "      \"semester\": \"Fall 2023\",\n",
    "      \"credits\": 3,\n",
    "      \"grade\": \"A\"\n",
    "    }\n",
    "  ]\n",
    "} \n",
    "\n",
    "Provide a query to the following prompt using Mongo Query Language\n",
    "\n",
    "Write a query to return all students who took “Intro to Big Data” in Spring 2023 and got at least a “B+” grade.  Only return the students' first and last names and their email addresses. Return the results sorted by students' last names and then by first names.\n",
    "\n",
    "\"\"\"\n",
    "response = generate_rag_response(query, search_embeddings(query, EMBEDDING_MODEL), LLM_MODEL)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = []\n",
    "\n",
    "for query in QUERIES:\n",
    "    print(query)\n",
    "    response = generate_rag_response(query, search_embeddings(query, EMBEDDING_MODEL), LLM_MODEL)\n",
    "    print(response)\n",
    "    print('-----------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
