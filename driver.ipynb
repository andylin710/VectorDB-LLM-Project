{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kibbl\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "\n",
      "llama3.2:latest sentence-transformers/all-mpnet-base-v2\n",
      "Clearing existing Redis store...\n",
      "Redis store cleared.\n",
      "Index created successfully.\n",
      "Processing PDFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [11:08<00:00, 41.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 668.3863 seconds\n",
      "Peak memory usage: 38.59 MiB\n",
      "\n",
      "---Done processing PDFs---\n",
      "\n",
      "Query: What is the difference between a list where memory is contiguously allocated and a list where linked structures are used?\n",
      "Based on the provided context, I can answer your query accurately.\n",
      "\n",
      "The main difference between a list where memory is contiguously allocated and a list where linked structures are used lies in how the records (or elements) are stored and accessed.\n",
      "\n",
      "In a **contiguous allocation** approach:\n",
      "\n",
      "* All n * x bytes of memory are allocated as a single chunk.\n",
      "* Each record takes up x bytes of memory, leaving some extra space between each record.\n",
      "* This means that all records are stored contiguously in memory, with no gaps or links between them.\n",
      "\n",
      "On the other hand, in a **linked list** approach:\n",
      "\n",
      "* Each record needs to store not only its own data but also additional space for 1 or 2 memory addresses (points to the next element).\n",
      "* The linked structure consists of individual nodes that point to each other, forming a chain.\n",
      "* This means that there are gaps between records in memory, and each node points to the next one.\n",
      "\n",
      "In summary, contiguous allocation stores all records contiguously in memory with no links between them, while linked structures store each record separately with additional space for pointers to the next element.\n",
      "Time elapsed: 54.164 seconds\n",
      "---------------------------\n",
      "Query: When are linked lists faster than contiguously-allocated lists? \n",
      "Linked lists are faster than contiguously-allocated lists for random insertions, not random access.\n",
      "Time elapsed: 16.1565 seconds\n",
      "---------------------------\n",
      "Query: Add 23 to the AVL Tree below.  What imbalance case is created with inserting 23?\n",
      "\n",
      "\t\t  30\n",
      "\t     /  \\\n",
      "\t    25   35\n",
      "\t   /\n",
      "     20\t\n",
      "To add 23 to the AVL tree, we need to perform the insertion process.\n",
      "\n",
      "The inserted node will be:\n",
      "     20\n",
      "    / \\\n",
      "   23 25\n",
      "  /   / \\\n",
      "15   30 35\n",
      "\n",
      "\n",
      "However, this is not an AVL tree because the difference in heights of its subtrees at node 30 (the height is 1) and node 35 (the height is -1) is more than 1.\n",
      "Time elapsed: 79.5276 seconds\n",
      "---------------------------\n",
      "Query: Why is a B+ Tree a better than an AVL tree when indexing a large dataset?\n",
      "I don't know. The provided context does not mention B+ Trees directly, and there is no information about their comparison to AVL trees in the given texts. However, I can provide general information about B+ Trees and their advantages.\n",
      "\n",
      "B+ Trees are often preferred over AVL Trees for indexing large datasets due to their design features that cater to disk storage and queries with multiple range searches. Here's why:\n",
      "\n",
      "1. **Higher Branching Factor**: B+ Trees have a higher branching factor than AVL Trees, which means they can store more keys in each node before needing to split. This reduces the number of disk I/O operations required to search for a key.\n",
      "\n",
      "2. **Node Ordering and Keys**: In B+ Trees, keys are stored at leaf nodes and pointers to child nodes are stored in parent nodes. Each node has a fixed number of children (defined by its order), and keys are ordered such that they can be efficiently searched without having to access every node.\n",
      "\n",
      "3. **Disk Efficiency**: B-Tree operations prioritize disk I/O efficiency, which is critical when dealing with large datasets that must be stored on disk. The design ensures that as few disk accesses as possible are required for any query or operation.\n",
      "\n",
      "4. **Efficient Range Searches**: Because B+ Trees allow multiple keys to be stored in each leaf node and the keys are ordered, they can efficiently support range queries (queries that find all keys within a given range). This feature makes them particularly well-suited for indexing large datasets where such searches are common.\n",
      "\n",
      "In contrast, AVL Trees maintain balance but might incur higher overhead due to the balancing operations required to keep the tree nearly balanced. While both data structures can be used for indexing and searching, B+ Trees are often preferred in scenarios where disk I/O efficiency is critical, especially when dealing with very large datasets that require efficient range queries.\n",
      "Time elapsed: 105.8049 seconds\n",
      "---------------------------\n",
      "Query: What is disk-based indexing and why is it important for database systems?\n",
      "Disk-based indexing is an indexing technique where the index is stored on disk, alongside the actual data. In a relational database system, indexing allows databases to quickly locate specific records or data based on a particular column or set of columns.\n",
      "\n",
      " Disk-based indexing is considered important in database systems because it provides faster searching and retrieval of data compared to linear scan of an entire column.\n",
      "Time elapsed: 37.87 seconds\n",
      "---------------------------\n",
      "Query: In the context of a relational database system, what is a transaction? \n",
      "In the context of a relational database system, a transaction is a sequence of one or more CRUD (Create, Read, Update, Delete) operations performed as a single, logical unit of work. It ensures data integrity, error recovery, concurrency control, and reliable data storage by treating the entire sequence as either fully executed (COMMIT) or failing entirely (ROLLBACK or ABORT).\n",
      "Time elapsed: 43.8103 seconds\n",
      "---------------------------\n",
      "Query: Succinctly describe the four components of ACID compliant transactions. \n",
      "Based on the provided context, I can succinctly describe the four components of ACID (Atomicity, Consistency, Isolation, and Durability) compliant transactions as follows:\n",
      "\n",
      "1. **Atomicity**: A transaction is treated as an atomic unit, meaning it is fully executed or not executed at all.\n",
      "2. **Consistency**: A transaction takes a database from one consistent state to another consistent state, ensuring that all data meets integrity constraints.\n",
      "3. **Isolation** (not explicitly mentioned in the provided context, but inferred): This component ensures that concurrent transactions do not interfere with each other's execution.\n",
      "4. **Durability**: Once a transaction is completed and committed successfully, its changes are permanent, even in the event of a system failure.\n",
      "\n",
      "Note: Isolation is not explicitly mentioned in the provided context, but it can be inferred as a necessary component of ACID compliance.\n",
      "Time elapsed: 41.3431 seconds\n",
      "---------------------------\n",
      "Query: Why does the CAP principle not make sense when applied to a single-node MongoDB instance? \n",
      "The CAP principle doesn't make sense when applied to a single-node MongoDB instance because it is based on the assumption of distributed systems, where multiple nodes are involved in providing the three guarantees. In a single-node system like MongoDB, all three guarantees can be achieved simultaneously.\n",
      "\n",
      "Consistency refers to ensuring that all nodes have the same data. Since there's only one node, consistency is inherently met as the data resides on the local machine.\n",
      "\n",
      "Availability refers to ensuring that every request receives a non-error response. With a single node, it's guaranteed that any incoming requests will receive a response (it might be an error if the server is down, but not due to data inconsistencies).\n",
      "\n",
      "Partition Tolerance isn't applicable in this context since a single-node system doesn't encounter network issues that could partition the data across different nodes.\n",
      "\n",
      "However, MongoDB does sacrifice some of these guarantees for the sake of performance and scalability. For instance, it uses an eventual consistency model which means writes may be visible on other instances after a delay (which violates consistency), or reads might return stale data if the write hasn't been replicated yet to other instances (violating availability).\n",
      "Time elapsed: 40.2445 seconds\n",
      "---------------------------\n",
      "Query: Describe the differences between horizontal and vertical scaling. \n",
      "Based on the provided context, I can describe the differences between horizontal and vertical scaling.\n",
      "\n",
      "Horizontal scaling refers to increasing the capacity of a system by adding more computers or machines to it, in order to handle increased loads or demands. This is often achieved through distributed computing models, where each computer operates concurrently with other computers to provide a single, unified service to users. Distributed systems are characterized by independent computers that fail individually and lack a shared global clock.\n",
      "\n",
      "Vertical scaling, on the other hand, involves increasing the power and capabilities of individual computers within a system, rather than adding new machines. This approach is often easier to implement, as it doesn't require significant changes to the underlying architecture of the system. However, vertical scaling has practical and financial limits, particularly when dealing with high-availability demands.\n",
      "\n",
      "In summary, the key differences between horizontal and vertical scaling are:\n",
      "\n",
      "* Horizontal scaling involves adding more computers to a system to increase capacity, while vertical scaling increases the capabilities of individual computers within the system.\n",
      "* Horizontal scaling is often achieved through distributed computing models, whereas vertical scaling is a more traditional approach that doesn't require significant changes to the system's architecture.\n",
      "\n",
      "I hope this answers your query accurately!\n",
      "Time elapsed: 42.2722 seconds\n",
      "---------------------------\n",
      "Query: Briefly describe how a key/value store can be used as a feature store. \n",
      "A key-value store can be used as a feature store by leveraging its simplicity, scalability, and fast retrieval capabilities. Here's how:\n",
      "\n",
      "1. **Simple data model**: Key-value stores have an extremely simple data model, which makes it easy to define and manage features. Features can be stored as key-value pairs, with the feature name as the key and the corresponding value (e.g., a string or number) as the value.\n",
      "2. **Scalability**: Key-value stores are designed for horizontal scaling, making it easy to add more nodes to handle increasing amounts of data. This scalability is essential for a feature store that may need to store and retrieve large numbers of features.\n",
      "3. **Fast retrieval**: Key-value stores provide fast retrieval capabilities, allowing users to quickly fetch features by their name (key). This makes it efficient for applications to use the stored features in real-time or near-real-time.\n",
      "4. **CRUD operations**: Key-value stores are well-suited for simple CRUD (Create, Read, Update, Delete) operations, which is typical for a feature store that may need to create, update, and retrieve features.\n",
      "\n",
      "By using a key-value store as a feature store, organizations can leverage its simplicity, scalability, and fast retrieval capabilities to manage their features efficiently.\n",
      "Time elapsed: 46.6584 seconds\n",
      "---------------------------\n",
      "Query: When was Redis originally released? \n",
      "I don't know. The provided context doesn't mention the release date of Redis. However, I can tell you that Redis is a widely used open-source, in-memory data store that was first released in 2005 by Salvatore Sanfilippo and Sergio Ribola.\n",
      "Time elapsed: 15.2049 seconds\n",
      "---------------------------\n",
      "Query: In Redis, what is the difference between the INC and INCR commands?\n",
      "I don't know. The provided context doesn't mention the INC and INCR commands. It only talks about publish/subscribe commands, save commands, info commands, and others. If you provide more context or information about the INC and INCR commands, I can try to help you better.\n",
      "Time elapsed: 54.4627 seconds\n",
      "---------------------------\n",
      "Query: What are the benefits of BSON over JSON in MongoDB?\n",
      "According to the context, BSON (Binary JSON) has several benefits over regular JSON:\n",
      "\n",
      "1. It allows you to work with types not allowed in regular JSON, such as date and binary data.\n",
      "2. It keeps space overhead to a minimum.\n",
      "3. It is designed to be easily traversed, which is vitally important for document DBs.\n",
      "4. It is efficient in encoding and decoding.\n",
      "\n",
      "These benefits suggest that BSON provides more flexibility and efficiency than regular JSON, making it a better choice for MongoDB's binary-encoded serialization of JSON-like documents.\n",
      "Time elapsed: 49.1383 seconds\n",
      "---------------------------\n",
      "Query: Write a Mongo query based on the movies data set that returns the titles of all movies released between 2010 and 2015 from the suspense genre? \n",
      "Based on the provided context, it seems like the MongoDB queries are using a different syntax than what you're asking for. The syntax used in your query is more similar to SQL.\n",
      "\n",
      "To answer your question accurately, I would suggest rewriting the query based on the \"find\" method in Mongo:\n",
      "\n",
      "```bash\n",
      "db.movies.find({\n",
      "    year: { $gte: 2010, $lte: 2015 },\n",
      "    genres: 'Suspense'\n",
      "})\n",
      "```\n",
      "\n",
      "This query will return all movies from the `movies` collection that were released between 2010 and 2015 (inclusive) and have a genre of \"Suspense\".\n",
      "\n",
      "Please note that you can adjust this query as per your requirements.\n",
      "Time elapsed: 39.0498 seconds\n",
      "---------------------------\n",
      "Query: What does the $nin operator mean in a Mongo query?\n",
      "The $nin operator is used to specify an array of values that are not included in a document. In other words, it's the opposite of the $in operator.\n",
      "\n",
      "For example, if you use `$in` with an array like `[ \"PG\", \"PG-13\" ]`, MongoDB will return documents where the `rated` field matches any value in that array.\n",
      "\n",
      "Using `$nin`, you would specify an array of values that should be excluded from the match, like this: `{ rated: { $nin: [ \"PG\", \"PG-13\" ] } }`. This would return documents where the `rated` field does not match either \"PG\" or \"PG-13\".\n",
      "\n",
      "So, in essence, `$nin` is used to filter out specific values from a collection.\n",
      "Time elapsed: 26.9668 seconds\n",
      "---------------------------\n",
      "Results saved to roland_redis_test_results.csv\n",
      "\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "\n",
      "llama3.2:latest sentence-transformers/all-mpnet-base-v2\n",
      "Clearing existing Redis store...\n",
      "Redis store cleared.\n",
      "Index created successfully.\n",
      "Processing PDFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [11:20<00:00, 42.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 680.3414 seconds\n",
      "Peak memory usage: 29.98 MiB\n",
      "\n",
      "---Done processing PDFs---\n",
      "\n",
      "Query: What is the difference between a list where memory is contiguously allocated and a list where linked structures are used?\n",
      "According to the context, the main difference between a list where memory is contiguous (also known as a contiguous allocation or array) and a list where linked structures are used is:\n",
      "\n",
      "- In a contiguous allocation, all records are stored in a single block of memory, which can lead to more efficient access and manipulation of data, especially when the number of elements is large. However, this comes with the trade-off that modifications may be slower since changing one element requires shifting all subsequent elements down.\n",
      "\n",
      "- In contrast, linked lists use additional space for pointers (or memory addresses) between each record. This allows individual records to be inserted or deleted at any position without affecting the rest of the list, making it more flexible and efficient for insertion or deletion operations at both the front and end of the list. However, this flexibility comes with a cost in terms of memory usage since additional space is needed for these pointers.\n",
      "\n",
      "In summary, contiguous allocation is beneficial when you primarily read data from your list (especially in scenarios where data isn't frequently modified), while linked structures are advantageous when dynamic insertions or deletions at various positions are necessary.\n",
      "Time elapsed: 53.4852 seconds\n",
      "---------------------------\n",
      "Query: When are linked lists faster than contiguously-allocated lists? \n",
      "Linked lists are faster than contiguously-allocated lists for random insertions, not random access.\n",
      "Time elapsed: 20.2884 seconds\n",
      "---------------------------\n",
      "Query: Add 23 to the AVL Tree below.  What imbalance case is created with inserting 23?\n",
      "\n",
      "\t\t  30\n",
      "\t     /  \\\n",
      "\t    25   35\n",
      "\t   /\n",
      "     20\t\n",
      "To add 23 to the AVL tree, we need to insert it as a new node.\n",
      "\n",
      "The current tree is:\n",
      "```\n",
      "     30\n",
      "    /  \\\n",
      "   25   35\n",
      "  /    \n",
      " 20   \n",
      "```\n",
      "\n",
      "After inserting 23, the new tree becomes:\n",
      "```\n",
      "     30\n",
      "    /  \\\n",
      "   25   35\n",
      "  / \\ \n",
      "20  23\n",
      "```\n",
      "\n",
      "Now, let's analyze the imbalance. We need to compare the heights of the left and right subtrees.\n",
      "\n",
      "The height of the left subtree (rooted at 20) is 1.\n",
      "The height of the right subtree (rooted at 25) is 0.\n",
      "The height of the root node (30) is not relevant here, as we're interested in the balance between the two subtrees.\n",
      "\n",
      "Since the difference in heights is greater than 2 (|0-1| = 1), this case does not create an imbalance.\n",
      "\n",
      "However, let's consider the immediate parent of the inserted node (23). The height of its left subtree (rooted at 20) is 1 and the right subtree has a negative height (-1).\n",
      "\n",
      "The difference in heights between these two subtrees is indeed greater than 2. However, we are interested in the case that can create an imbalance after inserting a new node. \n",
      "\n",
      "In this specific case, the insertion of 23 creates no imbalance by itself.\n",
      "\n",
      "However, if we consider all the nodes above it (including the root), then comparing their heights with each other could reveal an imbalance case.\n",
      "\n",
      "One way to do that is considering the height difference between the subtrees containing the keys 30 and 25. In this case, there is not an imbalance created at this level either.\n",
      "Time elapsed: 166.6467 seconds\n",
      "---------------------------\n",
      "Query: Why is a B+ Tree a better than an AVL tree when indexing a large dataset?\n",
      "Based on the provided context, I don't see any information about B+ Trees being compared to AVL Trees in terms of performance or suitability for indexing large datasets. The text only provides background information on AVL Trees and briefly mentions other data structures like B-Trees.\n",
      "\n",
      "However, based on general knowledge, B+ Trees are often considered better than AVL Trees for indexing large datasets due to their ability to efficiently manage multiple keys per node and reduce the number of disk I/O operations required to search or insert data. This is because B+ Trees can use a higher branching factor (typically 3-4) compared to AVL Trees, which allows them to navigate the tree more efficiently and reduce the average number of disk accesses needed.\n",
      "\n",
      "In contrast, AVL Trees are more focused on maintaining balance and have a lower branching factor (typically 2), which makes them better suited for scenarios where data access patterns are more sequential or predictable. However, when it comes to indexing large datasets with varying access patterns, B+ Trees' ability to manage multiple keys per node and reduce disk I/O operations can provide better performance.\n",
      "\n",
      "Please note that this is a general understanding and might not be specific to the exact context provided.\n",
      "Time elapsed: 112.9332 seconds\n",
      "---------------------------\n",
      "Query: What is disk-based indexing and why is it important for database systems?\n",
      "According to the provided context, disk-based indexing is a technique used in database systems to improve query performance by allowing for faster searching of specific values.\n",
      "\n",
      "In essence, disk-based indexing allows data to be stored on disk sorted by both id and specialVal (a unique value) simultaneously. This is important because it enables faster searching for a specific specialVal than a linear scan of the entire column.\n",
      "\n",
      "By using disk-based indexing, database systems can reduce the need for linear scans of individual columns, which can be time-consuming and inefficient. This technique helps to improve overall system performance by enabling faster data retrieval and reducing storage requirements.\n",
      "\n",
      "However, it's worth noting that storing data on disk sorted by both id and specialVal at the same time requires duplicating data, which can lead to space inefficiency. To address this issue, an external data structure is often used to support faster searching by specialVal than a linear scan alone.\n",
      "Time elapsed: 46.2167 seconds\n",
      "---------------------------\n",
      "Query: In the context of a relational database system, what is a transaction? \n",
      "In the context of a relational database system, a transaction is a sequence of one or more CRUD operations (Create, Read, Update, Delete) performed as a single, logical unit of work. Either the entire sequence succeeds (COMMIT) and the changes are permanent, or the entire sequence fails (ROLLBACK or ABORT) and the database is returned to its previous state, ensuring data integrity, error recovery, concurrency control, reliable data storage, and simplified error handling.\n",
      "Time elapsed: 42.6476 seconds\n",
      "---------------------------\n",
      "Query: Succinctly describe the four components of ACID compliant transactions. \n",
      "Based on the provided context, I can succinctly describe the four components of ACID (Atomicity, Consistency, Isolation, and Durability) compliant transactions as follows:\n",
      "\n",
      "1. Atomicity: A transaction is treated as an atomic unit, meaning it's fully executed or not executed at all.\n",
      "2. Consistency: A transaction takes a database from one consistent state to another consistent state, ensuring all data meets integrity constraints.\n",
      "3. Isolation: A pessimistic concurrency model that prevents conflicts by locking resources until a transaction is complete, similar to borrowing a book from a library where no one else can have it during that time.\n",
      "4. Durability: Once a transaction is completed and committed successfully, its changes are permanent, even in the event of a system failure.\n",
      "\n",
      "These components work together to ensure reliable and consistent data management.\n",
      "Time elapsed: 37.8902 seconds\n",
      "---------------------------\n",
      "Query: Why does the CAP principle not make sense when applied to a single-node MongoDB instance? \n",
      "The CAP principle doesn't apply to a single-node MongoDB instance because it is based on the idea that a distributed system can simultaneously provide more than two of the three guarantees (consistency, availability, and partition tolerance) but not all three.\n",
      "\n",
      "A single-node MongoDB instance provides consistency and availability by default, as data is stored locally and retrieved directly from the node. Since there's only one node, there's no partitioning to worry about, which means the third guarantee (partition tolerance) doesn't apply in the same way it would with a distributed system.\n",
      "\n",
      "In other words, a single-node MongoDB instance doesn't need to make trade-offs between consistency, availability, and partition tolerance because it can satisfy all three guarantees without any of them being relevant.\n",
      "Time elapsed: 30.1624 seconds\n",
      "---------------------------\n",
      "Query: Describe the differences between horizontal and vertical scaling. \n",
      "According to the provided context, the main difference between horizontal and vertical scaling is:\n",
      "\n",
      "Horizontal Scaling (also known as \"Scaling Out\"): This involves adding more machines or nodes to a system to increase its capacity, usually by distributing workload across multiple computers. It allows for more efficient use of resources and can make distributed computing models less problematic.\n",
      "\n",
      "Vertical Scaling (also known as \"Scaling Up\"): This involves increasing the power or performance of individual machines within a system, often through hardware upgrades or more powerful systems, in order to meet growing demands without necessarily modifying the underlying architecture.\n",
      "\n",
      "In summary, horizontal scaling involves adding new nodes to increase capacity, while vertical scaling involves upgrading existing nodes to increase performance.\n",
      "Time elapsed: 29.5879 seconds\n",
      "---------------------------\n",
      "Query: Briefly describe how a key/value store can be used as a feature store. \n",
      "Based on the provided context, a key-value store can be used as a feature store because it is designed around simplicity, with a simple data model and limited operations (CRUD). This makes it well-suited for storing and retrieving features in a straightforward manner. Additionally, the key-value store's focus on scalability and horizontal scaling allows it to easily handle large amounts of data and scale horizontally by adding more nodes as needed. As a feature store, it can be used to store, manage, and retrieve features in a simple and efficient way, making it suitable for use cases where simplicity and low latency are important.\n",
      "Time elapsed: 28.8464 seconds\n",
      "---------------------------\n",
      "Query: When was Redis originally released? \n",
      "I don't know. The context provided does not contain information about the release date of Redis.\n",
      "Time elapsed: 9.7001 seconds\n",
      "---------------------------\n",
      "Query: In Redis, what is the difference between the INC and INCR commands?\n",
      "I don't know. The provided context does not mention the INC and INCR commands. It only provides information on various Redis commands such as SAVE, BGSAVE, PUBLISH, SUBSCRIBE, UNSUBSCRIBE, PUNSUBSCRIBE, and others, but not INC or INCR. If you provide more relevant context, I'd be happy to help.\n",
      "Time elapsed: 53.2582 seconds\n",
      "---------------------------\n",
      "Query: What are the benefits of BSON over JSON in MongoDB?\n",
      "According to the provided context, BSON (Binary JSON) has several benefits over regular JSON:\n",
      "\n",
      "1. **Supports extended types**: BSON allows for binary-encoded serialization of a JSON-like document structure, supporting extended types not part of basic JSON, such as Date and BinaryData.\n",
      "2. **Lightweight**: BSON keeps space overhead to a minimum, making it more efficient in terms of storage requirements.\n",
      "3. **Traversable**: BSON is designed to be easily traversed, which is vital for a document DB.\n",
      "4. **Efficient encoding and decoding**: BSON enables efficient encoding and decoding, which is important for a document DB.\n",
      "\n",
      "These benefits make BSON a suitable choice for use with MongoDB, where it is the native data format.\n",
      "Time elapsed: 48.1507 seconds\n",
      "---------------------------\n",
      "Query: Write a Mongo query based on the movies data set that returns the titles of all movies released between 2010 and 2015 from the suspense genre? \n",
      "Based on the provided context, I'll attempt to answer your query.\n",
      "\n",
      "The relevant chunk of MongoDB queries is:\n",
      "\n",
      "```bash\n",
      "db.movies.find({ year: { $gte: 2010, $lte: 2015 }, genres: \"Suspense\" })\n",
      "```\n",
      "\n",
      "This query will return all movies from the `movies` collection that were released between 2010 and 2015 (inclusive) and have a genre of \"Suspense\".\n",
      "Time elapsed: 33.1432 seconds\n",
      "---------------------------\n",
      "Query: What does the $nin operator mean in a Mongo query?\n",
      "Based on the provided context, I can answer your question accurately.\n",
      "\n",
      "The `$nin` operator is used to find documents where a field is not equal to any of the specified values. It's the negation of the `$in` operator, which finds documents where a field is equal to one of the specified values.\n",
      "\n",
      "In the example you provided from page 23 of \"Document DBs and Mongo.pdf\", the `$nin` operator is used in the query:\n",
      "\n",
      "```javascript\n",
      "db.movies.find({ rated: { $nin: [\"PG\", \"PG-13\"] } })\n",
      "```\n",
      "\n",
      "This will find all documents in the `movies` collection where the `rated` field is not either `\"PG\"` or `\"PG-13\"`.\n",
      "Time elapsed: 28.1328 seconds\n",
      "---------------------------\n",
      "Results saved to roland_redis_test_results.csv\n",
      "\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "\n",
      "llama3.2:latest nomic-ai/nomic-embed-text-v1\n",
      "Clearing existing Redis store...\n",
      "Redis store cleared.\n",
      "Index created successfully.\n",
      "Processing PDFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]c:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kibbl\\.cache\\huggingface\\hub\\models--nomic-ai--nomic-embed-text-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Loading nomic-ai/nomic-embed-text-v1 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(LLM_MODEL, EMBEDDING_MODEL)\n\u001b[1;32m---> 35\u001b[0m \u001b[43mrun_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUERIES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEMBEDDING_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLLM_MODEL\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kibbl\\Desktop\\Schoolwork\\Projects\\Llama Cheat Sheet\\VectorDB-LLM-Project\\redis_pipeline.py:318\u001b[0m, in \u001b[0;36mrun_test\u001b[1;34m(queries, embedding_model, llm_model, chunk_size, overlap)\u001b[0m\n\u001b[0;32m    315\u001b[0m create_hnsw_index()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing PDFs...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 318\u001b[0m index_elapsed, index_memory \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_pdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAll_Slides/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---Done processing PDFs---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# define csv file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kibbl\\Desktop\\Schoolwork\\Projects\\Llama Cheat Sheet\\VectorDB-LLM-Project\\redis_pipeline.py:208\u001b[0m, in \u001b[0;36mprocess_pdfs\u001b[1;34m(data_dir, model, chunk_size, overlap)\u001b[0m\n\u001b[0;32m    206\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m split_text_into_chunks(text, chunk_size, overlap)\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m chunk_index, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m--> 208\u001b[0m                 embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m                 store_embedding(\n\u001b[0;32m    210\u001b[0m                     file\u001b[38;5;241m=\u001b[39mfile_name,\n\u001b[0;32m    211\u001b[0m                     page\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(page_num),\n\u001b[0;32m    212\u001b[0m                     chunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(chunk),\n\u001b[0;32m    213\u001b[0m                     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    214\u001b[0m                 )\n\u001b[0;32m    216\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\kibbl\\Desktop\\Schoolwork\\Projects\\Llama Cheat Sheet\\VectorDB-LLM-Project\\redis_pipeline.py:95\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text, embedding_model)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, embedding_model) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    Generate an embedding via a specified model. \u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:308\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    299\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[0;32m    302\u001b[0m     model_name_or_path,\n\u001b[0;32m    303\u001b[0m     token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    307\u001b[0m ):\n\u001b[1;32m--> 308\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    321\u001b[0m         model_name_or_path,\n\u001b[0;32m    322\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    330\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1739\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1741\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[1;32mc:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:80\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 80\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
      "File \u001b[1;32mc:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:145\u001b[0m, in \u001b[0;36mTransformer._load_config\u001b[1;34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1078\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1076\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1077\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n\u001b[1;32m-> 1078\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m   1083\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:679\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[0;32m    676\u001b[0m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[1;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    680\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires you to execute the configuration file in that\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    683\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[1;31mValueError\u001b[0m: Loading nomic-ai/nomic-embed-text-v1 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "# from qdrant_pipeline import run_test\n",
    "from redis_pipeline import run_test\n",
    "\n",
    "QUERIES = [\n",
    "'What is the difference between a list where memory is contiguously allocated and a list where linked structures are used?',\n",
    "'When are linked lists faster than contiguously-allocated lists? ',\n",
    "\"\"\"Add 23 to the AVL Tree below.  What imbalance case is created with inserting 23?\n",
    "\n",
    "\t\t  30\n",
    "\t     /  \\\\\n",
    "\t    25   35\n",
    "\t   /\n",
    "     20\t\"\"\",\n",
    "'Why is a B+ Tree a better than an AVL tree when indexing a large dataset?',\n",
    "'What is disk-based indexing and why is it important for database systems?'  ,\n",
    "'In the context of a relational database system, what is a transaction? ' ,\n",
    "'Succinctly describe the four components of ACID compliant transactions. ',\n",
    "'Why does the CAP principle not make sense when applied to a single-node MongoDB instance? ',\n",
    "'Describe the differences between horizontal and vertical scaling. ',\n",
    "'Briefly describe how a key/value store can be used as a feature store. ',\n",
    "'When was Redis originally released? ',\n",
    "'In Redis, what is the difference between the INC and INCR commands?' ,\n",
    "'What are the benefits of BSON over JSON in MongoDB?',\n",
    "'Write a Mongo query based on the movies data set that returns the titles of all movies released between 2010 and 2015 from the suspense genre? ',\n",
    "'What does the $nin operator mean in a Mongo query?'\n",
    "]\n",
    "\n",
    "# LLM_MODEL = 'llama3.2:latest'\n",
    "# EMBEDDING_MODEL = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "for LLM_MODEL in ['llama3.2:latest', 'gemma3:latest']:\n",
    "\tfor EMBEDDING_MODEL in ['sentence-transformers/all-mpnet-base-v2', 'sentence-transformers/all-mpnet-base-v2' ,'nomic-ai/nomic-embed-text-v1']:\n",
    "\t\tprint('\\n---------------------\\n---------------------\\n---------------------\\n')\n",
    "\t\tprint(LLM_MODEL, EMBEDDING_MODEL)\n",
    "\t\trun_test(QUERIES, EMBEDDING_MODEL, LLM_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
