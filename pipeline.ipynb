{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kibbl\\anaconda3\\envs\\captcha\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kibbl\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kibbl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama               # Ollama\n",
    "import redis                # Redis\n",
    "import numpy as np          # Duh\n",
    "\n",
    "import pypdf                # PDF reader\n",
    "from tqdm import tqdm       # Progress bar bc I'm impatient\n",
    "import os                   # Navigate folders\n",
    "import time                 # Timing\n",
    "import tracemalloc          # Memory Usage\n",
    "\n",
    "import re                   # Text preprocessing stuff\n",
    "import string               # More text preprocessing\n",
    "import nltk                 # Tokenization\n",
    "\n",
    "from sentence_transformers import SentenceTransformer       # Embedding Model\n",
    "from collections import Counter     # Simple counting dictionary\n",
    "from redis.commands.search.query import Query\n",
    "\n",
    "VECTOR_DIM = 768\n",
    "INDEX_NAME = \"embedding_index\"\n",
    "DOC_PREFIX = \"slides:\"\n",
    "DISTANCE_METRIC = \"COSINE\"\n",
    "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# Run these commands if stuff goes wacky\n",
    "# docker run -d --name redis-stack -p 6379:6379 redis/redis-stack\n",
    "# ollama pull nomic-embed-text\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper preprocessing functions\n",
    "\n",
    "def normalize_text(text, case_senstive=False):\n",
    "\n",
    "    # Normalizes case if need be\n",
    "    if case_senstive:\n",
    "        text = text.lower()\n",
    "\n",
    "    # Removes whitespace\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, method='word'):\n",
    "\n",
    "    # Very basic text normalization\n",
    "    text = normalize_text(text)\n",
    "\n",
    "    if method == 'word':\n",
    "    \n",
    "        # Tokenization\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords if need be\n",
    "        # tokens = remove_stopwords(tokens)\n",
    "\n",
    "        # Replaces wacky symbols (like stylized bullets) with <SYM> token if need be\n",
    "        # tokens = [\"<SYM>\" if re.fullmatch(r\"[^\\w\\d\" + re.escape(string.punctuation) + \"]\", token) else token for token in tokens]\n",
    "\n",
    "        # Replaces words that show up only once with <UNK> token if need be\n",
    "        # rare = [item[0] for item in Counter(tokens).items() if item[1] == 1]\n",
    "        # tokens = ['<UNK>' if token in rare else token for token in tokens]\n",
    "\n",
    "        # Replaces pure numbers with <NUM> token if need be\n",
    "        # tokens = ['<NUM>' if token.isdigit() else token for token in tokens]\n",
    "\n",
    "        # Removes punctuation marks\n",
    "        # tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    elif method == 'sent':\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "        # Preprocessing similar to regular word preprocessing if need be\n",
    "        for i in range(len(tokens)):\n",
    "            sent = tokens[i]\n",
    "            sent = ' '.join(preprocess_text(sent))\n",
    "            tokens[i] = sent\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap=0):\n",
    "    chunks = []\n",
    "    for start in range(len(text) // (chunk_size-overlap) + 1):\n",
    "        chunks.append(text[start * (chunk_size-overlap) : (start+1) * (chunk_size-overlap)])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to clear the redis vector store\n",
    "def clear_redis_store(redis_client):\n",
    "    print(\"Clearing existing Redis store...\")\n",
    "    redis_client.flushdb()\n",
    "    print(\"Redis store cleared.\")\n",
    "\n",
    "# Create an index in Redis\n",
    "def create_hnsw_index(redis_client):\n",
    "    try:\n",
    "        redis_client.execute_command(f\"FT.DROPINDEX {INDEX_NAME} DD\")\n",
    "    except redis.exceptions.ResponseError:\n",
    "        pass\n",
    "\n",
    "    redis_client.execute_command(\n",
    "        f\"\"\"\n",
    "        FT.CREATE {INDEX_NAME} ON HASH PREFIX 1 {DOC_PREFIX}\n",
    "        SCHEMA text TEXT\n",
    "        embedding VECTOR HNSW 6 DIM {VECTOR_DIM} TYPE FLOAT32 DISTANCE_METRIC {DISTANCE_METRIC}\n",
    "        \"\"\"\n",
    "    )\n",
    "    print(\"Index created successfully.\")\n",
    "\n",
    "# Generate an embedding using nomic-embed-text\n",
    "def get_embedding(text: str, model) -> list:\n",
    "    response = model.encode(text)\n",
    "    return response\n",
    "\n",
    "def store_embedding(redis_client, file: str, chunk: str, embedding: list):\n",
    "    key = f\"{DOC_PREFIX}{file}_chunk_{chunk}\"\n",
    "    redis_client.hset(\n",
    "        key,\n",
    "        mapping={\n",
    "            \"file\": file,\n",
    "            \"chunk\": chunk,\n",
    "            \"embedding\": np.array(\n",
    "                embedding, dtype=np.float32\n",
    "            ).tobytes(),  \n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_embeddings(redis_client, embedding_model, query, top_k=3):\n",
    "\n",
    "    query_embedding = get_embedding(query, embedding_model)\n",
    "\n",
    "    # Convert embedding to bytes for Redis search\n",
    "    query_vector = np.array(query_embedding, dtype=np.float32).tobytes()\n",
    "\n",
    "    try:\n",
    "        # Construct the vector similarity search query\n",
    "        # Use a more standard RediSearch vector search syntax\n",
    "        # q = Query(\"*\").sort_by(\"embedding\", query_vector)\n",
    "\n",
    "        q = (\n",
    "            Query(\"*=>[KNN 5 @embedding $vec AS vector_distance]\")\n",
    "            .sort_by(\"vector_distance\")\n",
    "            .return_fields(\"id\", \"file\", \"chunk\", \"vector_distance\")\n",
    "            .dialect(2)\n",
    "        )\n",
    "\n",
    "        # Perform the search\n",
    "        results = redis_client.ft(INDEX_NAME).search(\n",
    "            q, query_params={\"vec\": query_vector}\n",
    "        )\n",
    "\n",
    "        # Transform results into the expected format\n",
    "        top_results = [\n",
    "            {\n",
    "                \"file\": result.file,\n",
    "                \"chunk\": result.chunk,\n",
    "                \"similarity\": result.vector_distance,\n",
    "            }\n",
    "            for result in results.docs\n",
    "        ][:top_k]\n",
    "\n",
    "        # Print results for debugging\n",
    "        for result in top_results:\n",
    "            print(\n",
    "                f\"---> File: {result['file']}, Page: {result['page']}, Chunk: {result['chunk']}\"\n",
    "            )\n",
    "\n",
    "        return top_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing Redis store...\n",
      "Redis store cleared.\n",
      "Index created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:23<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 23.6348 seconds\n",
      "Peak memory usage: 48.72 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Client\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Model\n",
    "embedding_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Clear and create index\n",
    "clear_redis_store(redis_client)\n",
    "create_hnsw_index(redis_client)\n",
    "\n",
    "# Start time / memory check\n",
    "tracemalloc.start()\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over every slide\n",
    "for doc in tqdm(os.listdir('Slides')):\n",
    "\n",
    "    # Read text\n",
    "    reader = pypdf.PdfReader(f'Slides/{doc}')\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    # Create chunks\n",
    "    tokens = preprocess_text(text)\n",
    "    chunks = chunk_text(tokens, 500)\n",
    "\n",
    "    # Add chunks to redis\n",
    "    for i in range(len(chunks)):\n",
    "        chunk = ' '.join(chunks[i])\n",
    "        embed = get_embedding(chunk, embedding_model)\n",
    "        store_embedding(redis_client, doc, i, embed)\n",
    "\n",
    "# End time / memory check\n",
    "elapsed = time.time() - start_time\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f'Time elapsed: {round(elapsed, 4)} seconds')\n",
    "print(f\"Peak memory usage: {peak / 1024**2:.2f} MiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_embeddings(redis_client, embedding_model, 'Binary Search Trees', top_k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
